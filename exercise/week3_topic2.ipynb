{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNBxeB1oScVXTZKt27I5D8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tangYang7/GAI/blob/main/exercise/week3_topic2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explain Cross Entropy and Kullback-Leibler Divergence\n",
        "### Cross Entropy\n",
        "- Measures the similarity between two probability distributions, commonly used to **evaluate how well the predicted probability distribution matches the ground truth labels.**\n",
        "\n",
        "### KL Divergence\n",
        "- Measures the difference between two probability distributions, typically used to **quantify the relative entropy loss between a given distribution and a target distribution.**"
      ],
      "metadata": {
        "id": "iVkK5fnyAAuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Problem Definition\n",
        "- If we have the label for N classess, and in a random sample, the true distribution (ground truth) annotes $$ P = [P(1), P(2), ... , P(i),  ..., P(N)] $$\n",
        "- We have a model Î¸, and we got the prediction $$ Q = [Q(1), Q(2), ..., Q(i), ..., Q(N)] $$\n",
        "- H(P) is the entropy of the true distribution (a constant value).\n",
        "\n",
        "### 2. Cross Entropy\n",
        "- The formula for cross entropy is:\n",
        "\n",
        "$$\n",
        "H (P, Q) = \\sum_i P(i) logQ(i)\n",
        "$$"
      ],
      "metadata": {
        "id": "KShs9Gx_BB1P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UFtce2u1_3GK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "p = torch.tensor([0.2, 0.3, 0.5])\n",
        "q1 = torch.tensor([0.1, 0.4, 0.5])\n",
        "q2 = torch.tensor([0.1, 0.1, 0.8])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CE1 = torch.sum(p * torch.log(q1))\n",
        "CE2 = torch.sum(p * torch.log(q2))\n",
        "print(CE1)\n",
        "print(CE2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OKo6z1FItvM",
        "outputId": "552fe211-41f8-4766-8e7f-27d58d99f065"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1.0820)\n",
            "tensor(-1.2629)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. KL Divergence\n",
        "- KL divergence measures the relative entropy between two probability distributions:\n",
        "$$\n",
        "D_{KL}(P||Q)=\\sum_i P(i)log \\frac{P(i)}{Q(i)}\n",
        "$$\n",
        "\n",
        "- What's more, we can expand the formula:\n",
        "$$\n",
        "D_{KL}(P||Q)=\\sum_i P(i)log \\frac{P(i)}{Q(i)} = \\sum_i P(i)logP(i) - \\sum_i P(i)logQ(i) \\\\ = H(P) - H(P, Q)\n",
        "$$\n",
        "- Since H(P) is constant, minimizing cross entropy is equivalent to minimizing KL divergence."
      ],
      "metadata": {
        "id": "N4jhJOH3FDus"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KLD1 = torch.sum(p * torch.log(p / q1))\n",
        "KLD2 = torch.sum(p * torch.log(p / q2))\n",
        "print(KLD1)\n",
        "print(KLD2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAlaZiorGioT",
        "outputId": "e32a99e2-e71c-48ad-ee3b-48f79e912e09"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0523)\n",
            "tensor(0.2332)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate H(P)\n",
        "H_P = torch.sum(p * torch.log(p))\n",
        "print(H_P)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-L3_le-JQ-N",
        "outputId": "db757d1e-0a22-41cd-d0d5-bef149282363"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(-1.0297)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Calcuate H(P) - H(P, Q), and we get the same value as the one  in KL Divergence"
      ],
      "metadata": {
        "id": "a6VB5HvBJn76"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(H_P - CE1)\n",
        "print(H_P - CE2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPHHFQWoJYZn",
        "outputId": "f2807d2e-bc2f-44d1-84da-d68606e512a1"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0523)\n",
            "tensor(0.2332)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. What situations do we use Cross Entropy or KL Divergence?\n",
        "- As mentioned in the beginning, Cross Entropy focuses on the **similarity between P and Q** while KL Divergence measures how **difference between P and Q**.\n",
        "\n",
        "- In deep learning, softmax is used to output probability distributions, and cross entropy is used to compute loss.\n",
        "- Take Knowledge Distillation for examples, KL Divergence can be used to train a smaller model (student) from a larger model (teacher), which means they approximate a distribution of the prediction of the teacher.\n",
        "- In most machine learning applications, we minimize cross entropy, while KL divergence is often used for probability distribution learning."
      ],
      "metadata": {
        "id": "4sStgaPcLrRk"
      }
    }
  ]
}